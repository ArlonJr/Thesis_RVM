{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d4995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_images_from_directory(directory, label, target_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    files = glob.glob(os.path.join(directory, \"*.jpg\"))\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            img = cv2.imread(file)\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {file}\")\n",
    "                continue\n",
    "                \n",
    "            # Convert BGR to RGB (cv2 loads as BGR, but MobileNet expects RGB)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Resize image\n",
    "            img = cv2.resize(img, target_size).astype(np.float32)\n",
    "            \n",
    "            # Preprocess for MobileNetV2\n",
    "            img = preprocess_input(img)\n",
    "            \n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Load data from exactly two folders\n",
    "def load_binary_data():\n",
    "    base_path = \"data/\"\n",
    "    \n",
    "    # Define your two categories - modify these to match your folder names\n",
    "    category_a = \"plastic\"  # First category folder name\n",
    "    category_b = \"waste\"  # Second category folder name\n",
    "    \n",
    "    # Initialize lists\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Load category A images (label 0)\n",
    "    category_a_path = os.path.join(base_path, category_a)\n",
    "    if not os.path.exists(category_a_path):\n",
    "        print(f\"Error: Directory {category_a_path} does not exist\")\n",
    "        return None, None\n",
    "            \n",
    "    print(f\"Loading images from {category_a}...\")\n",
    "    images, labels = load_images_from_directory(category_a_path, 0)  # Label 0 for category A\n",
    "    all_images.extend(images)\n",
    "    all_labels.extend(labels)\n",
    "    \n",
    "    # Load category B images (label 1)\n",
    "    category_b_path = os.path.join(base_path, category_b)\n",
    "    if not os.path.exists(category_b_path):\n",
    "        print(f\"Error: Directory {category_b_path} does not exist\")\n",
    "        return None, None\n",
    "            \n",
    "    print(f\"Loading images from {category_b}...\")\n",
    "    images, labels = load_images_from_directory(category_b_path, 1)  # Label 1 for category B\n",
    "    all_images.extend(images)\n",
    "    all_labels.extend(labels)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(all_images)\n",
    "    y = np.array(all_labels)\n",
    "    \n",
    "    print(f\"Loaded {len(X)} images with shape {X.shape}\")\n",
    "    \n",
    "    # Check class distribution\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for u, c in zip(unique, counts):\n",
    "        class_name = category_b if u == 1 else category_a\n",
    "        print(f\"Class {u} ({class_name}): {c} samples\")\n",
    "    \n",
    "    return X, y, [category_a, category_b]  # Return category names for later use\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading dataset...\")\n",
    "X, y, categories = load_binary_data()\n",
    "\n",
    "# Check if data loaded successfully\n",
    "if X is None or y is None:\n",
    "    print(\"Failed to load data. Please check the folder names and structure.\")\n",
    "    exit()\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y),\n",
    "    y=y\n",
    ")\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Class weights:\", class_weights_dict)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create data generators with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator()  # No augmentation for validation data\n",
    "\n",
    "# Fix for \"input ran out of data\" warning by setting shuffle=True and setting proper batch sizes\n",
    "batch_size = 32\n",
    "\n",
    "# Calculate steps_per_epoch correctly\n",
    "train_samples = len(X_train)\n",
    "val_samples = len(X_val)\n",
    "steps_per_epoch = train_samples // batch_size\n",
    "validation_steps = val_samples // batch_size\n",
    "\n",
    "# Ensure at least one step\n",
    "steps_per_epoch = max(1, steps_per_epoch)\n",
    "validation_steps = max(1, validation_steps)\n",
    "\n",
    "# Configure the generators to shuffle data\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train, y_train, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow(\n",
    "    X_val, y_val, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Training with {steps_per_epoch} steps per epoch, {validation_steps} validation steps\")\n",
    "\n",
    "# Build model with proper output layer for binary classification\n",
    "print(\"Building model...\")\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Initially freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add classification head\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# Binary output with sigmoid activation\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile model with binary crossentropy loss\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# Add callbacks for better training\n",
    "checkpoint_path = \"best_model_binary.keras\"\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=1e-6, verbose=1),\n",
    "    ModelCheckpoint(checkpoint_path, save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Step 1: Train with frozen base model layers\n",
    "print(\"Phase 1: Training with frozen base model...\")\n",
    "history1 = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=15,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 2: Unfreeze the base model and fine-tune with a lower learning rate\n",
    "print(\"Phase 2: Fine-tuning with unfrozen base model...\")\n",
    "# Unfreeze some layers in the base model\n",
    "for layer in base_model.layers[-40:]:  # Unfreeze last 40 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# Continue training with unfrozen layers\n",
    "history2 = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=25,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights_dict,\n",
    "    verbose=1,\n",
    "    initial_epoch=len(history1.history['accuracy'])  # Start from the last epoch\n",
    ")\n",
    "\n",
    "# Load the best model\n",
    "model = tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "# Evaluate the model on full validation set\n",
    "val_loss, val_acc, val_auc, val_precision, val_recall = model.evaluate(X_val, y_val, batch_size=batch_size)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"Validation precision: {val_precision:.4f}\")\n",
    "print(f\"Validation recall: {val_recall:.4f}\")\n",
    "\n",
    "# Generate predictions for confusion matrix\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('binary_confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_classes, target_names=categories))\n",
    "\n",
    "# Create ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('binary_roc_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Check what keys are available in history objects\n",
    "print(\"Available keys in history1:\", history1.history.keys())\n",
    "print(\"Available keys in history2:\", history2.history.keys())\n",
    "\n",
    "# Combine histories from both training phases with proper key handling\n",
    "history_combined = {}\n",
    "\n",
    "# Add metrics that we're sure exist\n",
    "history_combined['accuracy'] = history1.history['accuracy'] + history2.history['accuracy']\n",
    "history_combined['val_accuracy'] = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
    "history_combined['loss'] = history1.history['loss'] + history2.history['loss']\n",
    "history_combined['val_loss'] = history1.history['val_loss'] + history2.history['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Accuracy subplot\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history_combined['accuracy'])\n",
    "plt.plot(history_combined['val_accuracy'])\n",
    "plt.axvline(x=len(history1.history['accuracy']), color='r', linestyle='--')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation', 'Phase Change'], loc='lower right')\n",
    "\n",
    "# Loss subplot\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history_combined['loss'])\n",
    "plt.plot(history_combined['val_loss'])\n",
    "plt.axvline(x=len(history1.history['loss']), color='r', linestyle='--')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation', 'Phase Change'], loc='upper right')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('binary_training_history.png')\n",
    "plt.show()\n",
    "\n",
    "# Create visualizations for sample predictions\n",
    "def visualize_predictions(X_val, y_val, model, categories, num_samples=10):\n",
    "    # Get random samples\n",
    "    indices = np.random.choice(range(len(X_val)), num_samples, replace=False)\n",
    "    images = X_val[indices]\n",
    "    true_labels = y_val[indices]\n",
    "    \n",
    "    # Get predictions\n",
    "    preds = model.predict(images)\n",
    "    pred_classes = (preds > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i, idx in enumerate(range(num_samples)):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        \n",
    "        # Convert image back for visualization\n",
    "        img = images[idx].copy()\n",
    "        # Undo preprocessing (approximate)\n",
    "        img = (img + 1) / 2\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        plt.imshow(img)\n",
    "        correct = true_labels[idx] == pred_classes[idx]\n",
    "        color = \"green\" if correct else \"red\"\n",
    "        \n",
    "        true_label_str = categories[true_labels[idx]]\n",
    "        pred_label_str = categories[pred_classes[idx]]\n",
    "        \n",
    "        conf = preds[idx][0] if pred_classes[idx] == 1 else 1 - preds[idx][0]\n",
    "        \n",
    "        plt.title(f\"True: {true_label_str}\\nPred: {pred_label_str}\\nConf: {conf:.2f}\", \n",
    "                 color=color)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('binary_sample_predictions.png')\n",
    "    plt.show()\n",
    "\n",
    "# Sample visualization\n",
    "print(\"Creating visualization of sample predictions...\")\n",
    "visualize_predictions(X_val, y_val, model, categories)\n",
    "\n",
    "# Save model\n",
    "model.save(f\"binary_classification_{categories[0]}_vs_{categories[1]}.keras\")\n",
    "print(f\"Model saved as binary_classification_{categories[0]}_vs_{categories[1]}.keras\")\n",
    "\n",
    "print(\"Binary classification model training and analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9554d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
